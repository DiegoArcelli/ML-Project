{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ecdc2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 11:30:10.886170: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-23 11:30:19.225145: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-23 11:30:44.182435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-23 11:30:44.182656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-23 11:30:44.182672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import keras_tuner\n",
    "import keras\n",
    "from keras import layers\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from math import ceil\n",
    "import pickle\n",
    "\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import layers\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks import ModelSummary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import Metric\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import pytorch_lightning as pl\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "import logging\n",
    "import os\n",
    "logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "logging.getLogger(\"lightning\").propagate = False\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca9df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(curve_1, label_1, curve_2=None, label_2=None):\n",
    "    if curve_1 is not None: \n",
    "        plt.plot(curve_1, label = label_1)\n",
    "    if curve_2 is not None: \n",
    "        plt.plot(curve_2, label = label_2)   \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ac1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLearningRateScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_init, lr_final, epochs, last_epoch=-1):\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_final = lr_final\n",
    "        self.epochs = epochs\n",
    "        super().__init__(optimizer, last_epoch=last_epoch)\n",
    "    \n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        epoch = self.last_epoch\n",
    "        if epoch < self.epochs:\n",
    "            alpha = epoch / self.epochs\n",
    "            lr = self.lr_init * (1 - alpha) + alpha*self.lr_final\n",
    "        else:\n",
    "            lr = self.lr_final\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    \n",
    "\n",
    "def get_activation_function(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif name == \"linear\":\n",
    "        return None\n",
    "\n",
    "def mee(y_true, y_pred):\n",
    "    return (y_true - y_pred).square().sum(axis=1).sqrt().mean()\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return (y_true - y_pred).square().sum(axis=1).mean()\n",
    "    \n",
    "class MEE(Metric):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"result\", default=torch.tensor(0.0))\n",
    "        \n",
    "    def update(self, y_pred, y_true):\n",
    "        self.result += mee(y_true, y_pred)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.result\n",
    "    \n",
    "\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        task = config[\"task\"]\n",
    "        self.lr = config[\"learning_rate\"]\n",
    "        self.momentum = config[\"momentum\"]\n",
    "        self.nesterov = config[\"nesterov\"]\n",
    "        init = config[\"initialization\"]\n",
    "        reg_method = config[\"regularization\"][\"method\"]\n",
    "        self.reg_method = reg_method\n",
    "        self.lr_decay = config[\"learning_rate_decay\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if reg_method != None:\n",
    "            value = config[\"regularization\"][\"value\"]\n",
    "            self.reg_value = value\n",
    "        \n",
    "        self.task = task\n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        units = config[\"units_1\"]\n",
    "        \n",
    "        self.model.add_module(\"linear_input\", nn.Linear(input_dim, units))\n",
    "        \n",
    "        n_layers = 0\n",
    "        for key in config.keys():\n",
    "            if key.startswith(\"units_\"):\n",
    "                n_layers += 1\n",
    "                \n",
    "        for n_layer in range(1, n_layers+1):\n",
    "            out_units = config[f\"units_{n_layer}\"]\n",
    "            self.model.add_module(f\"linear_{n_layer}\", nn.Linear(units, out_units))\n",
    "            activation = get_activation_function(config[f\"activations_{n_layer}\"])\n",
    "            if activation != None:\n",
    "                self.model.add_module(f\"activations_{n_layer}\", activation)\n",
    "            units = out_units\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            self.model.add_module(\"layer_output\", nn.Linear(units, output_dim))\n",
    "            self.model.add_module(\"activations_output\", nn.Sigmoid())\n",
    "            self.metric = Accuracy()\n",
    "            self.metric_name = \"accuracy\"\n",
    "        \n",
    "        if task == \"regression\":\n",
    "            self.model.add_module(\"layer_output\", nn.Linear(units, output_dim))\n",
    "            self.metric = MEE()\n",
    "            self.metric_name = \"mee\"\n",
    "                    \n",
    "        if init[\"type\"] == \"uniform\":\n",
    "            self.min = init[\"min\"]\n",
    "            self.max = init[\"max\"]\n",
    "            self.apply(self.init_weights_uniform)\n",
    "        elif init[\"type\"] == \"normal\":\n",
    "            self.mean = init[\"mean\"]\n",
    "            self.std = init[\"std\"]\n",
    "            self.apply(self.init_weights_normal)\n",
    "            \n",
    "            \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.dicts = []\n",
    "            \n",
    "            \n",
    "\n",
    "    def init_weights_uniform(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.uniform_(m.weight, self.min, self.max)\n",
    "            torch.nn.init.uniform_(m.bias, self.min, self.max)\n",
    "            # m.bias.data.fill_(0.01)\n",
    "            \n",
    "\n",
    "    def init_weights_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, self.mean, self.std)\n",
    "            torch.nn.init.normal_(m.bias, self.mean, self.std)\n",
    "            # m.bias.data.fill_(0.01)\n",
    "            \n",
    "            \n",
    "    def compute_metric(self, y_pred, y_true):\n",
    "        if self.task == \"classification\":\n",
    "            y_pred = y_pred.reshape(y_true.shape)  \n",
    "            return self.metric(y_pred, y_true.to(torch.int32))\n",
    "        elif self.task == \"regression\":\n",
    "            y_true = y_true.to(torch.float32)\n",
    "            return self.metric(y_pred, y_true)\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        if self.task == \"classification\":\n",
    "            y_pred = y_pred.reshape(y_true.shape)  \n",
    "            return self.loss(y_pred, y_true)\n",
    "        elif self.task == \"regression\":\n",
    "            y_true = y_true.to(torch.float32)\n",
    "            return self.loss(y_pred, y_true)\n",
    "        \n",
    "        \n",
    "    def get_regularization_term(self):\n",
    "        if self.reg_method != None:\n",
    "            \n",
    "            value = self.reg_value\n",
    "            p = 1 if self.reg_method == \"l1\" else 2\n",
    "            reg = 0\n",
    "            \n",
    "            for param in self.model.parameters():\n",
    "                reg += torch.linalg.norm(param, p)\n",
    "                \n",
    "            reg *= value\n",
    "            \n",
    "            return reg\n",
    "        \n",
    "        return 0\n",
    "            \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum, nesterov=self.nesterov)\n",
    "        if self.lr_decay != None:\n",
    "            decay_epochs = self.lr_decay[\"epochs\"]\n",
    "            lr_final = self.lr_decay[\"lr_final\"]\n",
    "            lr_init = self.lr\n",
    "            scheduler = LinearLearningRateScheduler(optimizer, lr_init, lr_final, decay_epochs)\n",
    "            return [optimizer], [scheduler]\n",
    "            \n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch \n",
    "        y_copy = y # Integer y for the accuracy\n",
    "        X = X.type(torch.float32)\n",
    "        y = y.type(torch.float32)  \n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # accuracy\n",
    "        acc = self.compute_metric(y_pred, y_copy)\n",
    "        # compute loss\n",
    "        reg = self.get_regularization_term()\n",
    "        loss = self.compute_loss(y_pred, y) + reg\n",
    "        self.log_dict({'loss': loss, f'{self.metric_name}': acc}, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def validation_step(self, validation_batch, batch_idx):\n",
    "        X, y = validation_batch\n",
    "        X = X.type(torch.float32)\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # compute metrics \n",
    "        acc = self.compute_metric(y_pred, y)\n",
    "        # compute loss\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        self.log_dict({'val_loss': loss, f'val_{self.metric_name}': acc}, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        X, y = test_batch\n",
    "        X = X.type(torch.float32)\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # compute metrics \n",
    "        acc = self.compute_metric(y_pred, y)\n",
    "        # compute loss\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        self.log_dict({'test_loss': loss, f'test_{self.metric_name}': acc}, on_epoch=True,on_step=False, prog_bar=True, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941b8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = None\n",
    "y_scaler = None\n",
    "\n",
    "def load_dataset(scale=True):\n",
    "    global x_scaler\n",
    "    global y_scaler\n",
    "    \n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    \n",
    "    train = pd.read_csv(\"../data/ML-CUP22-INTERNAL-TR.csv\", header=None, sep=\",\")\n",
    "    test = pd.read_csv(\"../data/ML-CUP22-INTERNAL-TS.csv\", header=None, sep=\",\")\n",
    "    train.drop(0, axis=1, inplace=True)\n",
    "    test.drop(0, axis=1, inplace=True)\n",
    "\n",
    "    x_train = train.iloc[:, :9].to_numpy().astype(np.float64)\n",
    "    y_train = train.iloc[:, 9:].to_numpy().astype(np.float64)\n",
    "    \n",
    "    test = pd.read_csv(\"../data/ML-CUP22-TS.csv\", header=None, sep=\",\")\n",
    "    test.drop(0, axis=1, inplace=True)\n",
    "\n",
    "    x_test = test.iloc[:, :9].to_numpy().astype(np.float64)    \n",
    "    \n",
    "    if scale:\n",
    "        x_scaler.fit(x_train)\n",
    "        x_train = x_scaler.transform(x_train)\n",
    "        x_test = x_scaler.transform(x_test)\n",
    "        \n",
    "        y_scaler.fit(y_train)\n",
    "        y_train = y_scaler.transform(y_train)        \n",
    "        \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a905d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blind_test_set_prediction(model, test, file_name):\n",
    "    global y_scaler\n",
    "    \n",
    "    if isinstance(model, MultiLayerPerceptron):\n",
    "        y = model(torch.from_numpy(test).to(torch.float32))\n",
    "        y = y.detach().numpy()\n",
    "    else:\n",
    "        y = model.predict(test)\n",
    "        \n",
    "    y = y_scaler.inverse_transform(y)\n",
    "    np.savetxt(f\"./../results/{file_name}\", y, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aba4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5679f2",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df718c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 11:31:25.005885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:25.835061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:25.835700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:25.866930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-23 11:31:25.867470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:25.868055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:25.868563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:34.509152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:34.522855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:34.523438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-23 11:31:34.523749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1619 MB memory:  -> device: 0, name: NVIDIA GeForce 930MX, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"./../models/keras_model.hdf5\", custom_objects={\"mee\": mee})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3ae4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 13s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "get_blind_test_set_prediction(model, test, \"keras_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99a546",
   "metadata": {},
   "source": [
    "# Randomized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2309c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./../models/randomized_model/', custom_objects={\"mee\": mee})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c69caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "get_blind_test_set_prediction(model, test, \"randomized_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a36739",
   "metadata": {},
   "source": [
    "#  Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f3c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./../models/pytorch_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4cef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_blind_test_set_prediction(model, test, \"pytroch_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371f9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
