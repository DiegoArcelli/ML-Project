{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37f696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import keras_tuner\n",
    "import keras\n",
    "from keras import layers\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from math import ceil\n",
    "import os\n",
    "\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(curve_1, label_1, curve_2=None, label_2=None):\n",
    "    if curve_1 is not None: \n",
    "        plt.plot(curve_1, label = label_1)\n",
    "    if curve_2 is not None: \n",
    "        plt.plot(curve_2, label = label_2)   \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60634f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = None\n",
    "y_scaler = None\n",
    "\n",
    "def load_monks(path, encode):\n",
    "    \n",
    "    train = pd.read_csv(f\"{path}.train\", header=None, sep=\" \")\n",
    "    test = pd.read_csv(f\"{path}.test\", header=None, sep=\" \")\n",
    "    \n",
    "    train.drop(0, axis=1, inplace=True)\n",
    "    test.drop(0, axis=1, inplace=True)\n",
    "    train.drop(8, axis=1, inplace=True)\n",
    "    test.drop(8, axis=1, inplace=True)\n",
    "    \n",
    "    y_train = train.iloc[:, 0].to_numpy().astype(np.float64)\n",
    "    x_train = train.iloc[:, 1:].to_numpy().astype(np.float64)\n",
    "    y_test = test.iloc[:, 0].to_numpy().astype(np.float64)\n",
    "    x_test = test.iloc[:, 1:].to_numpy().astype(np.float64)\n",
    "    \n",
    "        \n",
    "    if encode:\n",
    "        \n",
    "        encoder = MinMaxScaler()\n",
    "        \n",
    "        encoder.fit(x_train)\n",
    "        x_train = encoder.transform(x_train).toarray()\n",
    "        x_test = encoder.transform(x_test).toarray()\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"Loaded {path} dataset\")\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "\n",
    "def load_cup(scale):\n",
    "    global x_scaler\n",
    "    global y_scaler\n",
    "    \n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    # scaler = MinMaxScaler()\n",
    "    \n",
    "    train = pd.read_csv(\"../data/ML-CUP22-INTERNAL-TR.csv\", header=None, sep=\",\")\n",
    "    test = pd.read_csv(\"../data/ML-CUP22-INTERNAL-TS.csv\", header=None, sep=\",\")\n",
    "    train.drop(0, axis=1, inplace=True)\n",
    "    test.drop(0, axis=1, inplace=True)\n",
    "\n",
    "    x_train = train.iloc[:, :9].to_numpy().astype(np.float64)\n",
    "    y_train = train.iloc[:, 9:].to_numpy().astype(np.float64)\n",
    "\n",
    "    x_test = test.iloc[:, :9].to_numpy().astype(np.float64)\n",
    "    y_test = test.iloc[:, 9:].to_numpy().astype(np.float64) \n",
    "    # x_test_blind = test.to_numpy().astype(np.float64)\n",
    "    \n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=7)\n",
    "    \n",
    "    if scale:\n",
    "        x_scaler.fit(x_train)\n",
    "        x_train = x_scaler.transform(x_train)\n",
    "        x_test = x_scaler.transform(x_test)\n",
    "        \n",
    "        y_scaler.fit(y_train)\n",
    "        y_train = y_scaler.transform(y_train)\n",
    "        y_test = y_scaler.transform(y_test)\n",
    "        \n",
    "        \n",
    "    return x_train, y_train, x_test, y_test #, x_test_blind\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_dataset(dataset, encode=True):\n",
    "    assert dataset in [\"monks1\", \"monks2\", \"monks3\", \"cup\"]\n",
    "    \n",
    "    if dataset == \"monks1\":\n",
    "        return load_monks(\"./../data/monks-1\", encode)\n",
    "    elif dataset == \"monks2\":\n",
    "        return load_monks(\"./../data/monks-2\", encode)\n",
    "    elif dataset == \"monks3\":\n",
    "        return load_monks(\"./../data/monks-3\", encode)\n",
    "    else:\n",
    "        return load_cup(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a4b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomizedNetwork(keras.Model):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, layers, init=None, reg=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if init is None:\n",
    "            init = initializers.RandomNormal(mean=0, stddev=1, seed=None)\n",
    "        \n",
    "        self.reg = reg\n",
    "        \n",
    "        self.model = Sequential()\n",
    "                    \n",
    "            \n",
    "        units = layers[0][\"units\"]\n",
    "        activation = layers[0][\"activation\"]\n",
    "        self.model.add(Dense(units, input_dim=input_size, activation=activation,\n",
    "                    kernel_initializer = init, bias_initializer = init))\n",
    "        self.model.layers[0].trainable = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        for layer in layers[1:]:\n",
    "            activation = layer[\"activation\"]\n",
    "            units = layer[\"units\"]\n",
    "            self.model.add(Dense(units, activation=activation, \n",
    "                        kernel_initializer = init, bias_initializer = init))\n",
    "            self.model.layers[-1].trainable = False\n",
    "        \n",
    "        self.model.add(Dense(output_size, activation=\"linear\",\n",
    "                    kernel_initializer = init, bias_initializer = init))\n",
    "        \n",
    "        \n",
    "        self.compile(loss=\"mse\", metrics = mee)\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "    def lms_solve(self, x, y):\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        \n",
    "        # x_b = np.append(x, np.ones((n,1)), axis=-1)\n",
    "        for layer in self.model.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        \n",
    "        H = x.numpy()\n",
    "        H = np.append(H, np.ones((n,1)), axis=-1)\n",
    "        # H = self.model.layers[0](x).numpy()\n",
    "        \n",
    "        # if regularization is used apply direct solution\n",
    "        if self.reg is not None:\n",
    "            H_t = H.transpose()\n",
    "            H = np.matmul(H_t, H)\n",
    "            I = np.identity(H.shape[0])\n",
    "            H = H + self.reg*I\n",
    "            H_star = np.linalg.inv(H)\n",
    "            W = np.matmul(H_star, H_t)\n",
    "            W = np.matmul(W, y)\n",
    "        # if regularization is not used apply numpy LMS solver\n",
    "        else:\n",
    "            W, _, _, _ = np.linalg.lstsq(H.T.dot(H), H.T.dot(y))\n",
    "            \n",
    "        b = W[-1:][0]\n",
    "        W = W[:-1]\n",
    "        \n",
    "        # print(W.shape)\n",
    "        # print(b.shape)\n",
    "        # print(self.model.layers[-1].get_weights()[0].shape)\n",
    "        # print(self.model.layers[-1].get_weights()[1].shape)\n",
    "        self.model.layers[-1].set_weights([W, b])\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d8d98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_index(array):\n",
    "    n = len(array)\n",
    "    med_idx = np.argsort(array)[ceil((n-1)/2)]\n",
    "    # med_val = array[med_idx]\n",
    "    return med_idx #, med_val\n",
    "\n",
    "def mee(y_true_t, y_pred_t):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_true_t - y_pred_t), axis=1)))\n",
    "    \n",
    "    \n",
    "def mse(y_true_t, y_pred_t):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(y_true_t - y_pred_t), axis=1))\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    merged = {}\n",
    "    for d in dicts:\n",
    "        merged = {**merged, **d}\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_attributes_num(dataset):\n",
    "    return 1 if len(dataset.shape) == 1 else dataset.shape[1]\n",
    "\n",
    "\n",
    "def get_params_configurations(params):\n",
    "    \n",
    "    keys = list(params.keys())\n",
    "    keys.remove(\"layers\")\n",
    "    keys.remove(\"initialization\")\n",
    "    keys.remove(\"trials_train\")\n",
    "    keys.remove(\"trials_test\")\n",
    "    sorted_keys = sorted(keys)\n",
    "    arguments = []\n",
    "    for key in sorted_keys:\n",
    "        arguments.append(params[key])\n",
    "    \n",
    "    arguments = tuple(arguments)\n",
    "    all_params = list(itertools.product(*arguments))\n",
    "        \n",
    "    configurations = []\n",
    "    \n",
    "    for conf in all_params:\n",
    "        \n",
    "        dict_conf = {}\n",
    "        \n",
    "        for i in range(len(sorted_keys)):\n",
    "            dict_conf[sorted_keys[i]] = conf[i]\n",
    "        dict_conf[\"initialization\"] = params[\"initialization\"]\n",
    "        dict_conf[\"trials_train\"] = params[\"trials_train\"]\n",
    "        dict_conf[\"trials_test\"] = params[\"trials_test\"]\n",
    "        \n",
    "        configurations.append(dict_conf)\n",
    "        \n",
    "        \n",
    "    return configurations\n",
    "\n",
    "\n",
    "def get_layers_configurations(params, configurations):\n",
    "    layers = params[\"layers\"]\n",
    "    layers_params_confs = []\n",
    "    \n",
    "    for n_layer in range(len(layers)):\n",
    "        layers_params_confs.append([]) \n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        activations = layer[\"activations\"]\n",
    "        units = layer[\"units\"]\n",
    "        layer_confs = list(itertools.product(activations, units))\n",
    "        for conf in layer_confs:\n",
    "            conf_dict = {\n",
    "                f\"activations_{i+1}\": conf[0],\n",
    "                f\"units_{i+1}\": conf[1]\n",
    "            }\n",
    "            layers_params_confs[i].append(conf_dict)\n",
    "        \n",
    "    layers_confs = []\n",
    "    for i in range(len(layers_params_confs)):\n",
    "        confs = layers_params_confs[:i+1]\n",
    "        confs = tuple(confs)\n",
    "        confs = list(itertools.product(*confs))\n",
    "        for conf in confs:\n",
    "            layers_confs.append(merge_dicts(conf))\n",
    "                \n",
    "    return layers_confs\n",
    "\n",
    "\n",
    "\n",
    "def get_configurations(params):\n",
    "    \n",
    "    configurations = get_params_configurations(params)\n",
    "    layers_confs = get_layers_configurations(params, configurations)\n",
    "    configurations = list(itertools.product(configurations, layers_confs))\n",
    "    configurations_merged = []\n",
    "    \n",
    "    for conf in configurations:\n",
    "        configurations_merged.append(merge_dicts(conf))\n",
    "        \n",
    "    \n",
    "    return configurations_merged\n",
    "\n",
    "    \n",
    "\n",
    "def get_model(config, input_size, output_size):\n",
    "    \n",
    "    init_method = config[\"initialization\"][\"type\"]\n",
    "    reg = config[\"regularization\"]\n",
    "        \n",
    "    init = None\n",
    "    if init_method == \"uniform\":\n",
    "        min_val = config[\"initialization\"][\"min\"]\n",
    "        max_val = config[\"initialization\"][\"max\"]\n",
    "        init = initializers.RandomUniform(minval=min_val, maxval=max_val, seed=None)\n",
    "    if init_method == \"normal\":\n",
    "        mean = config[\"initialization\"][\"mean\"]\n",
    "        std = config[\"initialization\"][\"std\"]\n",
    "        init = initializers.RandomNormal(mean=mean, stddev=std, seed=None)\n",
    "        \n",
    "        \n",
    "    n_layers = 0\n",
    "    for key in config.keys():\n",
    "        if key.startswith(\"units_\"):\n",
    "            n_layers += 1\n",
    "    \n",
    "    layers = []\n",
    "    for l in range(1, n_layers+1):\n",
    "        activation = config[f\"activations_{l}\"]\n",
    "        n_units = config[f\"units_{l}\"]\n",
    "        layers.append({ \n",
    "            \"activation\": activation,\n",
    "            \"units\": n_units\n",
    "        })\n",
    "        \n",
    "    model = RandomizedNetwork(input_size, output_size, layers, init, reg)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "    \n",
    "\n",
    "def get_metrics(model, x_train, y_train, x_test=None, y_test=None):\n",
    "    train_loss, train_metric  = model.evaluate(x_train, y_train, verbose=0)\n",
    "    test_loss, test_metric  = model.evaluate(x_test, y_test, verbose=0) if x_test is not None else [None, None]\n",
    "    return train_loss, train_metric, test_loss, test_metric\n",
    "    \n",
    "\n",
    "def fit_model_k_fold(x_train, y_train, config, n_folds):\n",
    "    \n",
    "\n",
    "    train_metric = \"mee\"\n",
    "    val_metric = \"val_mee\"\n",
    "\n",
    "    trials = config[\"trials_train\"]\n",
    "\n",
    "    \n",
    "    kf = KFold(n_splits = n_folds, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "        \n",
    "        print(f\"\\nExecuting fold {fold}/{n_folds}\")\n",
    "        \n",
    "        input_size = get_attributes_num(x_train)\n",
    "        output_size = get_attributes_num(y_train)\n",
    "        \n",
    "                \n",
    "        x_train_fold = x_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "\n",
    "        x_val_fold = x_train[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        \n",
    "        trials_val_loss = []\n",
    "        trials_train_loss = []\n",
    "        trials_val_acc = []\n",
    "        trials_train_acc = []\n",
    "        \n",
    "        with tqdm(total=trials) as pbar:\n",
    "            for trial in range(trials):\n",
    "                model = get_model(config, input_size, output_size)\n",
    "                \n",
    "                model.lms_solve(x_train_fold, y_train_fold)\n",
    "                \n",
    "                trial_train_loss, trial_train_acc, trial_val_loss, trial_val_acc = get_metrics(model, x_train_fold, y_train_fold, x_val_fold, y_val_fold)\n",
    "\n",
    "                trials_val_loss.append(trial_val_loss)\n",
    "                trials_train_loss.append(trial_train_loss)\n",
    "                trials_val_acc.append(trial_val_acc)\n",
    "                trials_train_acc.append(trial_train_acc)\n",
    "\n",
    "                pbar.update(1)\n",
    "            \n",
    "        # get the median metrics among the trials\n",
    "        med_idx = median_index(trials_val_loss)\n",
    "        med_val_loss = trials_val_loss[med_idx]\n",
    "        med_train_loss = trials_train_loss[med_idx]\n",
    "        med_val_acc = trials_val_acc[med_idx]\n",
    "        med_train_acc = trials_train_acc[med_idx]\n",
    "        std_val_loss = np.std(trials_val_loss)\n",
    "        std_train_loss = np.std(trials_train_loss)\n",
    "        std_val_acc = np.std(trials_val_acc)\n",
    "        std_train_acc = np.std(trials_train_acc)\n",
    "\n",
    "        print(f\"Fold {fold}/{n_folds} median val_loss: {med_val_loss}, std val_loss {std_val_loss}\")\n",
    "        print(f\"Fold {fold}/{n_folds} median train_loss: {med_train_loss}, std train_loss {std_train_loss}\")\n",
    "        print(f\"Fold {fold}/{n_folds} median {val_metric}: {med_val_acc}, std {val_metric} {std_val_acc}\")\n",
    "        print(f\"Fold {fold}/{n_folds} median train_{train_metric}: {med_train_acc}, train_{train_metric} {std_train_acc}\")\n",
    "            \n",
    "        train_losses.append(med_train_loss)\n",
    "        val_losses.append(med_val_loss)\n",
    "        val_accs.append(med_val_acc)\n",
    "        train_accs.append(med_train_acc)\n",
    "        \n",
    "        fold += 1\n",
    "        \n",
    "    mean_train_loss = np.mean(train_losses)\n",
    "    mean_val_loss = np.mean(val_losses)\n",
    "    mean_train_acc = np.mean(train_accs)\n",
    "    mean_val_acc = np.mean(val_accs)\n",
    "    std_train_loss = np.std(train_losses)\n",
    "    std_val_loss = np.std(val_losses)\n",
    "    std_train_acc = np.std(train_accs)\n",
    "    std_val_acc = np.std(val_accs)\n",
    "    \n",
    "    print(f\"\\nMean val_loss: {mean_val_loss}, std val_loss: {std_val_loss}\")\n",
    "    print(f\"Mean train_loss: {mean_train_loss}, std train_loss: {std_train_loss}\")\n",
    "    print(f\"Mean {val_metric}: {mean_val_acc}, std {val_metric}: {std_val_acc}\")\n",
    "    print(f\"Mean train_{train_metric}: {mean_val_acc}, std train_{train_metric}: {std_train_acc}\")\n",
    "    \n",
    "    return mean_val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_model_hold_out(x_train, y_train, config, val_split):\n",
    "    \n",
    "    train_metric = \"mee\"\n",
    "    val_metric = \"val_mee\"\n",
    "        \n",
    "    trials = config[\"trials_train\"]\n",
    "    input_size = get_attributes_num(x_train)\n",
    "    output_size = get_attributes_num(y_train)\n",
    "        \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_split, shuffle=True)\n",
    "\n",
    "    \n",
    "    trials_val_loss = []\n",
    "    trials_train_loss = []\n",
    "    trials_val_acc = []\n",
    "    trials_train_acc = []\n",
    "        \n",
    "    with tqdm(total=trials) as pbar:\n",
    "        for trial in range(trials):            \n",
    "            model = get_model(config, input_size, output_size)\n",
    "                \n",
    "            model.lms_solve(x_train, y_train)\n",
    "\n",
    "            trial_train_loss, trial_train_acc, trial_val_loss, trial_val_acc = get_metrics(model, x_train, y_train, x_val, y_val)\n",
    "\n",
    "            trials_val_loss.append(trial_val_loss)\n",
    "            trials_train_loss.append(trial_train_loss)\n",
    "            trials_val_acc.append(trial_val_acc)\n",
    "            trials_train_acc.append(trial_train_acc)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # get the median metrics among the trials\n",
    "    med_idx = median_index(trials_val_loss)\n",
    "    med_val_loss = trials_val_loss[med_idx]\n",
    "    med_train_loss = trials_train_loss[med_idx]\n",
    "    med_val_acc = trials_val_acc[med_idx]\n",
    "    med_train_acc = trials_train_acc[med_idx]\n",
    "    std_val_loss = np.std(trials_val_loss)\n",
    "    std_train_loss = np.std(trials_train_loss)\n",
    "    std_val_acc = np.std(trials_val_acc)\n",
    "    std_train_acc = np.std(trials_train_acc)\n",
    "\n",
    "    print(f\"Median val_loss: {med_val_loss}, std val_loss {std_val_loss}\")\n",
    "    print(f\"Median train_loss: {med_train_loss}, std train_loss {std_train_loss}\")\n",
    "    print(f\"Median {val_metric}: {med_val_acc}, std {val_metric} {std_val_acc}\")\n",
    "    print(f\"Median train_{train_metric}: {med_train_acc}, train_{train_metric} {std_train_acc}\")\n",
    "    \n",
    "    return med_val_loss\n",
    "    \n",
    "    \n",
    "def fit_final_model(x_train, y_train, config):\n",
    "\n",
    "    train_metric = \"mee\"\n",
    "    val_metric = \"val_mee\"\n",
    "    train_loss = \"mse\"\n",
    "    val_loss = \"mse\"\n",
    "\n",
    "    trials = config[\"trials_test\"]\n",
    "\n",
    "            \n",
    "    input_size = get_attributes_num(x_train)\n",
    "    output_size = get_attributes_num(y_train)\n",
    "    \n",
    "    \n",
    "    trials_val_loss = []\n",
    "    trials_train_loss = []\n",
    "    trials_val_acc = []\n",
    "    trials_train_acc = []\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, stratify=None)\n",
    "\n",
    "    \n",
    "    with tqdm(total=trials) as pbar:\n",
    "        for trial in range(trials):\n",
    "            model = get_model(config, input_size, output_size)\n",
    "                \n",
    "            model.lms_solve(x_train, y_train)\n",
    "            \n",
    "            models.append(model)\n",
    "\n",
    "            trial_train_loss, trial_train_acc, trial_val_loss, trial_val_acc = get_metrics(model, x_train, y_train, x_val, y_val)\n",
    "\n",
    "            trials_val_loss.append(trial_val_loss)\n",
    "            trials_train_loss.append(trial_train_loss)\n",
    "            trials_val_acc.append(trial_val_acc)\n",
    "            trials_train_acc.append(trial_train_acc)\n",
    "\n",
    "            print(f\"Trial {trial+1}/{trials} {val_loss}: {trial_val_loss}\")\n",
    "            print(f\"Trial {trial+1}/{trials} train_{train_loss}: {trial_train_loss}\")\n",
    "            print(f\"Trial {trial+1}/{trials} {val_metric}: {trial_val_acc}\")\n",
    "            print(f\"Trial {trial+1}/{trials} train_{train_metric}: {trial_train_acc}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "        \n",
    "    med_idx = median_index(trials_val_loss)\n",
    "    med_val_loss = trials_val_loss[med_idx]\n",
    "    med_train_loss = trials_train_loss[med_idx]\n",
    "    med_val_acc = trials_val_acc[med_idx]\n",
    "    med_train_acc = trials_train_acc[med_idx]\n",
    "    std_val_loss = np.std(trials_val_loss)\n",
    "    std_train_loss = np.std(trials_train_loss)\n",
    "    std_val_acc = np.std(trials_val_acc)\n",
    "    std_train_acc = np.std(trials_train_acc)\n",
    "    \n",
    "    print(f\"\\nMedian {val_loss}: {med_val_loss}, std {val_loss}: {std_val_loss}\")\n",
    "    print(f\"Median train_{train_loss}: {med_train_loss}, std train_{train_loss}: {std_train_loss}\")\n",
    "    print(f\"Median {val_metric}: {med_val_acc}, std {val_metric}: {std_val_acc}\")\n",
    "    print(f\"Median train_{train_metric}: {med_train_acc}, std train_{train_metric} {std_train_acc}\")\n",
    "        \n",
    "    med_model = models[med_idx]\n",
    "    \n",
    "    \n",
    "    y_pred = med_model.predict(x_val)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred)\n",
    "    y_val = y_scaler.inverse_transform(y_val)\n",
    "\n",
    "\n",
    "\n",
    "    mse_val = mse(y_val, y_pred)\n",
    "    mee_val = mee(y_val, y_pred)\n",
    "\n",
    "    print(f\"\\nValidation MSE: {mse_val}\")\n",
    "    print(f\"Validation MEE: {mee_val}\")\n",
    "    \n",
    "    return med_model\n",
    "\n",
    "    \n",
    "def fit_model(x_train, y_train, config, validation):\n",
    "    if validation[\"type\"] == \"k-fold\":\n",
    "        val_loss = fit_model_k_fold(x_train, y_train, config, validation[\"n_folds\"])\n",
    "    elif validation[\"type\"] == \"hold-out\":\n",
    "        val_loss = fit_model_hold_out(x_train, y_train, config, validation[\"val_split\"])\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def assess_model(model, x_train, y_train, x_test, y_test, scale=True):\n",
    "    global y_scaler\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    \n",
    "    if scale: \n",
    "        y_train_pred = y_scaler.inverse_transform(y_train_pred)\n",
    "        y_train = y_scaler.inverse_transform(y_train)\n",
    "        y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "        y_test = y_scaler.inverse_transform(y_test)\n",
    "    \n",
    "    \n",
    "    mse_train = mse(y_train, y_train_pred)\n",
    "    mse_test = mse(y_test, y_test_pred)\n",
    "    \n",
    "    mee_train = mee(y_train, y_train_pred)\n",
    "    mee_test = mee(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Train MSE: {mse_train}\")\n",
    "    print(f\"Train MEE: {mee_train}\")\n",
    "    print(f\"Test MSE: {mse_test}\")\n",
    "    print(f\"Test MEE: {mee_test}\")\n",
    "    \n",
    "    \n",
    "def model_selection(configurations, x_train, y_train, validation):\n",
    "        \n",
    "    best_loss = float(\"inf\")\n",
    "    best_conf = None\n",
    "    \n",
    "    idx = 1\n",
    "    n_confs = len(configurations)\n",
    "    for config in configurations:\n",
    "\n",
    "        print(f\"Testing configuration {idx}/{n_confs}:\\n{config}\")\n",
    "        val_loss = fit_model(x_train, y_train, config, validation)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss;\n",
    "            best_conf = config\n",
    "        idx += 1\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Best configuration (loss {best_loss}):\\n{best_conf}\")\n",
    "    \n",
    "    return best_conf\n",
    "    \n",
    "\n",
    "def grid_search(params, x_train, y_train, validation={\"type\": \"hold-out\", \"val_split\": 0.2}):\n",
    "    configurations = get_configurations(params)    \n",
    "    best_conf = model_selection(configurations, x_train, y_train, validation)\n",
    "    model = fit_final_model(x_train, y_train, best_conf)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ede16f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = {\n",
    "    \"type\": \"normal\",\n",
    "    \"mean\": 0,\n",
    "    \"std\": 0.5\n",
    "}\n",
    "\n",
    "unif = {\n",
    "    \"type\": \"uniform\",\n",
    "    \"min\": -1,\n",
    "    \"max\": 1\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"trials_train\": 10,\n",
    "    \"trials_test\": 10,\n",
    "    \"initialization\": unif,\n",
    "    \"regularization\": [10**e for e in range(-5, 5)],\n",
    "    \"layers\": [\n",
    "        {\n",
    "            \"activations\": [\"tanh\", \"relu\", \"sigmoid\"],\n",
    "            \"units\": [2, 5, 10, 20, 30, 50, 75, 100, 200, 300, 500, 750, 1000],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"trials_train\": 1,\n",
    "    \"trials_test\": 10,\n",
    "    \"initialization\": unif,\n",
    "    \"regularization\": [1],\n",
    "    \"layers\": [\n",
    "        {\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"units\": [1000],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "k_fold_val = {\n",
    "    \"type\": \"k-fold\",\n",
    "    \"n_folds\": 5\n",
    "}\n",
    "\n",
    "hold_out_val = {\n",
    "    \"type\": \"hold-out\",\n",
    "    \"val_split\": 0.2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a98502",
   "metadata": {},
   "source": [
    "# ML Cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cebb8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1343, 9) (1343, 2)\n",
      "(149, 9) (149, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_dataset(\"cup\", True)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bb5d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration 1/1:\n",
      "{'regularization': 1, 'initialization': {'type': 'uniform', 'min': -1, 'max': 1}, 'trials_train': 1, 'trials_test': 10, 'activations_1': 'relu', 'units_1': 1000}\n",
      "\n",
      "Executing fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 median val_loss: 0.1437375694513321, std val_loss 0.0\n",
      "Fold 1/5 median train_loss: 0.030969373881816864, std train_loss 0.0\n",
      "Fold 1/5 median val_mee: 0.4149380922317505, std val_mee 0.0\n",
      "Fold 1/5 median train_mee: 0.19306999444961548, train_mee 0.0\n",
      "\n",
      "Executing fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 median val_loss: 0.143571138381958, std val_loss 0.0\n",
      "Fold 2/5 median train_loss: 0.03280431032180786, std train_loss 0.0\n",
      "Fold 2/5 median val_mee: 0.42286020517349243, std val_mee 0.0\n",
      "Fold 2/5 median train_mee: 0.19665436446666718, train_mee 0.0\n",
      "\n",
      "Executing fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 median val_loss: 0.13544487953186035, std val_loss 0.0\n",
      "Fold 3/5 median train_loss: 0.03256116434931755, std train_loss 0.0\n",
      "Fold 3/5 median val_mee: 0.4226539433002472, std val_mee 0.0\n",
      "Fold 3/5 median train_mee: 0.19133061170578003, train_mee 0.0\n",
      "\n",
      "Executing fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 median val_loss: 0.13657963275909424, std val_loss 0.0\n",
      "Fold 4/5 median train_loss: 0.03319530934095383, std train_loss 0.0\n",
      "Fold 4/5 median val_mee: 0.4227026402950287, std val_mee 0.0\n",
      "Fold 4/5 median train_mee: 0.1947777271270752, train_mee 0.0\n",
      "\n",
      "Executing fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 median val_loss: 0.11706779897212982, std val_loss 0.0\n",
      "Fold 5/5 median train_loss: 0.03365080803632736, std train_loss 0.0\n",
      "Fold 5/5 median val_mee: 0.3918558657169342, std val_mee 0.0\n",
      "Fold 5/5 median train_mee: 0.19531278312206268, train_mee 0.0\n",
      "\n",
      "Mean val_loss: 0.1352802038192749, std val_loss: 0.009733179884634461\n",
      "Mean train_loss: 0.032636193186044694, std train_loss: 0.0009115079220411504\n",
      "Mean val_mee: 0.4150021493434906, std val_mee: 0.011961196446119924\n",
      "Mean train_mee: 0.4150021493434906, std train_mee: 0.001849433893680655\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best configuration (loss 0.1352802038192749):\n",
      "{'regularization': 1, 'initialization': {'type': 'uniform', 'min': -1, 'max': 1}, 'trials_train': 1, 'trials_test': 10, 'activations_1': 'relu', 'units_1': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:00<00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10 mse: 0.123868428170681\n",
      "Trial 1/10 train_mse: 0.034376174211502075\n",
      "Trial 1/10 val_mee: 0.40519630908966064\n",
      "Trial 1/10 train_mee: 0.1979120373725891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 2/10 [00:01<00:04,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2/10 mse: 0.12516029179096222\n",
      "Trial 2/10 train_mse: 0.03441854193806648\n",
      "Trial 2/10 val_mee: 0.4049340486526489\n",
      "Trial 2/10 train_mee: 0.19934821128845215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [00:01<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3/10 mse: 0.12277016043663025\n",
      "Trial 3/10 train_mse: 0.034481633454561234\n",
      "Trial 3/10 val_mee: 0.3961482644081116\n",
      "Trial 3/10 train_mee: 0.19775868952274323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [00:02<00:03,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4/10 mse: 0.11591406911611557\n",
      "Trial 4/10 train_mse: 0.03328532725572586\n",
      "Trial 4/10 val_mee: 0.3853444457054138\n",
      "Trial 4/10 train_mee: 0.19669361412525177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5/10 mse: 0.10752330720424652\n",
      "Trial 5/10 train_mse: 0.03236180543899536\n",
      "Trial 5/10 val_mee: 0.37139633297920227\n",
      "Trial 5/10 train_mee: 0.1911664754152298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6/10 mse: 0.1263875663280487\n",
      "Trial 6/10 train_mse: 0.034190475940704346\n",
      "Trial 6/10 val_mee: 0.3997374176979065\n",
      "Trial 6/10 train_mee: 0.1993284374475479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7/10 mse: 0.13724975287914276\n",
      "Trial 7/10 train_mse: 0.03302721306681633\n",
      "Trial 7/10 val_mee: 0.41704413294792175\n",
      "Trial 7/10 train_mee: 0.19509008526802063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8/10 mse: 0.13063950836658478\n",
      "Trial 8/10 train_mse: 0.033768072724342346\n",
      "Trial 8/10 val_mee: 0.4072204530239105\n",
      "Trial 8/10 train_mee: 0.19736498594284058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9/10 mse: 0.1209135502576828\n",
      "Trial 9/10 train_mse: 0.0335017591714859\n",
      "Trial 9/10 val_mee: 0.3961687386035919\n",
      "Trial 9/10 train_mee: 0.19689971208572388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10/10 mse: 0.1169997826218605\n",
      "Trial 10/10 train_mse: 0.03367985412478447\n",
      "Trial 10/10 val_mee: 0.3882237672805786\n",
      "Trial 10/10 train_mee: 0.19690240919589996\n",
      "\n",
      "Median mse: 0.123868428170681, std mse: 0.007796146418593506\n",
      "Median train_mse: 0.034376174211502075, std train_mse: 0.0006545155219671824\n",
      "Median val_mee: 0.40519630908966064, std val_mee: 0.012302379844391144\n",
      "Median train_mee: 0.1979120373725891, std train_mee 0.002236780355612928\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "\n",
      "Validation MSE: 5.800400907906447\n",
      "Validation MEE: 2.046151854241932\n",
      "42/42 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step\n",
      "Train MSE: 2.3704457041932234\n",
      "Train MEE: 1.1806145705735602\n",
      "Test MSE: 6.240058560564201\n",
      "Test MEE: 2.111124574713496\n"
     ]
    }
   ],
   "source": [
    "model = grid_search(params, x_train, y_train, k_fold_val)\n",
    "assess_model(model, x_train, y_train, x_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./../models/randomized_model’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./../models/randomized_model/assets\n"
     ]
    }
   ],
   "source": [
    "os.system(\"mkdir ./../models/randomized_model\")\n",
    "model.save('./../models/randomized_model/' ,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1ce100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_size = 9\\noutput_size = 2\\n\\nconf = {\\'regularization\\': 10, \\'initialization\\': {\\'type\\': \\'normal\\', \\'mean\\': 0, \\'std\\': 1}, \\'trials_train\\': 5, \\'trials_test\\': 10, \\'activations_1\\': \\'tanh\\', \\'units_1\\': 1000}\\n\\nELM = get_model(conf, input_size, output_size)# RandomizedNetwork(input_size, output_size, layers, init, 0.1)\\nprint(ELM(x_train).shape)\\nprint(y_train.shape)\\noptimizer = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\\n#LM.compile(optimizer = optimizer, loss = \\'mean_squared_error\\', metrics = mee)\\nELM.summary()\\n\\nELM.lms_solve(x_train, y_train)\\nprint(\"Traing set accuracy\")\\nprint(ELM.evaluate(x_train, y_train))\\n\\nprint(\"\\nTest set accuracy\")\\nprint(ELM.evaluate(x_test, y_test))'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''input_size = 9\n",
    "output_size = 2\n",
    "\n",
    "conf = {'regularization': 10, 'initialization': {'type': 'normal', 'mean': 0, 'std': 1}, 'trials_train': 5, 'trials_test': 10, 'activations_1': 'tanh', 'units_1': 1000}\n",
    "\n",
    "ELM = get_model(conf, input_size, output_size)# RandomizedNetwork(input_size, output_size, layers, init, 0.1)\n",
    "print(ELM(x_train).shape)\n",
    "print(y_train.shape)\n",
    "optimizer = SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
    "#LM.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = mee)\n",
    "ELM.summary()\n",
    "\n",
    "ELM.lms_solve(x_train, y_train)\n",
    "print(\"Traing set accuracy\")\n",
    "print(ELM.evaluate(x_train, y_train))\n",
    "\n",
    "print(\"\\nTest set accuracy\")\n",
    "print(ELM.evaluate(x_test, y_test))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8763cc51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(y_train[:, \u001b[38;5;241m0\u001b[39m], y_train[:, \u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pred = scaler.inverse_transform(pred)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(y_train[:, 0], y_train[:, 1], label=\"True\")\n",
    "# pred = scaler.inverse_transform(pred)\n",
    "pred = model.predict(x_train)\n",
    "plt.scatter(pred[:, 0], pred[:, 1], label = \"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d84ed06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(y_test[:, \u001b[38;5;241m0\u001b[39m], y_test[:, \u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pred = scaler.inverse_transform(pred)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(y_test[:, 0], y_test[:, 1], label=\"True\")\n",
    "# pred = scaler.inverse_transform(pred)\n",
    "pred = model.predict(x_test)\n",
    "plt.scatter(pred[:, 0], pred[:, 1], label = \"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28332846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_loss, mee_loss  = ELM.evaluate(x_train, y_train, verbose=0)\n",
    "# print(mse_loss)\n",
    "# print(mee_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993f620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
